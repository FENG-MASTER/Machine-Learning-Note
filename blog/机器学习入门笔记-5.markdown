# 前言 #

这次笔记是针对第三周的分类算法的笔记

## 特征 ##

分类问题的特征:

1. 输出结果是离散的

## 种类 ##

1. **二元分类问题**
2. 多元分类问题

>这次笔记讨论的都是二元分类问题

## 线性回归与分类问题 ##

直接应用线性回归的模型套用到分类问题中是**不可行的**

原因:

1. 训练出来的模型会随着训练集的变化而变化(好的模型不应该随着训练集变化而变化,应该是固定的)
2. 根据模型计算出来的值可能比分类问题中的离散结果还要大(如离散结果应该只能1或者0,但会出现大于1的现象)



## 假设函数(二元分类问题) ##

既然前面视频已经了解大概机器学习算法的流程了,那我就只能帖假设函数出来了,其他细枝末节自己看视频,我这个只是笔记,画的是重点

### 逻辑函数 ###

又名S型函数(Sigmoid function),这个不是最终的假设函数,只是介绍下,下面是用到的其中一个表达式:

![](https://i.imgur.com/APZYMCW.jpg)

Log-Sigmoid

### 假设函数 ###

分类算法的假设函数由两部分组成,一个是矩阵乘法,一个就是逻辑函数

![](https://i.imgur.com/hamkggz.png)
 
![](https://i.imgur.com/mp4PMv9.png)


我们顺便来看个逻辑函数的图,加深下理解

![](https://i.imgur.com/QiYWgmd.png)

接下来我要解释下这个假设函数的意义,首先假设我们训练好了我们的模型,参数已经确定了,那么,这时候我们输入一个x想看下属于什么分类(0或者1),这个时候h假设函数会输出一个值,这个值介于0-1之间

表达的意思是:

**x这个值,属于1分类的可能性**

一般情况下,大于0.5,我们就说这个x属于1分类,小于0.5,我们就说这个x属于0分类


## 决策边界 ##

这个概念很简单,就是划开每个分类的那个边界

比如你已经训练好了模型

![](https://i.imgur.com/Yms7raC.png)

中间那个线就是决策边界,其实对应的是

![](https://i.imgur.com/g3K26fV.png)

这条线

## 代价函数 ##

二元分类问题模型的代价函数也有所变化,已经不是之前线性回归用的那个平方的代价函数了,这个的代价函数稍微复杂点


![](https://i.imgur.com/gmRzmij.png)




这个子代价函数cost()就需要简单解释下了

1. 当输入的训练集中的y等于1,则使用第一个式子计算代价
2. 当输入的训练集中的y等于0,则使用第二个式子计算代价


##### y=1时候那个式子的图像是怎么样的 #####

![](https://i.imgur.com/vAHS22l.png)

我对这个图的直观感受就是:

1. 当假设函数计算出来的值越接近1,代价越少;计算出来的值为1的时候,代价最小为0.
2. 当假设函数计算出来的值越远离1,代价函数越大;计算出来的值为0的时候,代价无穷大.

##### y=0时候那个式子的图像是怎么样的 #####

![](https://i.imgur.com/VyFH9I8.png)

不解释,同上,自己理解

## 简化代价函数 ##


上面写的这个代价函数写起来太复杂,还要两行,我们简化成一行

![](https://i.imgur.com/08zxvf6.png)

你问我这个式子和前面为什么是等价的?

当y=0的时候,没有前面那项,当y=1的时候,没有后面那项,所以就相等了(我觉得初中水平就可以理解).

## 梯度下降算法 ##

这部分没变,依旧是递归找出全局最小值点,我贴个式子上来吧,免得忘了.


![](https://i.imgur.com/bicfkkV.png)

要点:

1. 同步更新


## 优化最优化算法 ##

除了最简单的梯度下降算法外,还有其他更高级,更加复杂的最优化算法,但这些算法由于太复杂,并不推荐你自己手动的实现,一般都是直接使用别人写好的,直接调用就可以了.

一些高级最优化算法:
1. 共轭梯度法 
2. BFGS(变尺度法)
2. L-BFGS(限制变尺度法)

高级算法优点:
1. 不需要自己选择leaning rate(即α)
2. 一般比梯度下降法快

高级算法缺点:
1. 复杂,难易实现


### 用法 ###

总体来说:
1. 给出代价函数的计算方法
2. 给出代价函数对于每个特征量求导的结果


#### 例子 ####

##### 1. 我们先把上面说的东西定义成一个函数(怎么定义?看前面的笔记),最终定义成一个函数文件(costFunction.m) #####


	% 这个函数的功能就是根据输入的thete值,返回对应代价函数的结果 和 代价函数在每个特征分量上的导数值 %
	function [jVal,gradient]=costFunction(theta)
	
	jVal=(theta(1)-5)^2+(theta(2)-5)^2; 
	
	gradient=zeros(2,1);
	gradient(1)=2*(theta(1)-5);
	gradient(2)=2*(theta(2)-5);

 解释

 1. 第四行计算代价函数值对应的式子是:

 	![](https://i.imgur.com/F3fJGYI.png)

 2. 第7.8行对应代价函数分别在两个特征变量的导数的值对应的式子是:

 	![](https://i.imgur.com/Fv9N3xs.png)



##### 2. 定义相关参数 ##### 

输入:

	options=optimset('GradObj','on','MaxIter','100');
	%定义了一个参数对象,意思是打开GradObj模式,一会后面我代码我们会直接传一个所谓的GradObj进去,MaxIter表示最大迭代次数设置为100%


##### 3. 定义初始位置 #####

输入:

	initialTheta=zeros(2,1);
	% 定义2*1 的向量,全为0,我们用来当作起始探测位置 %

##### 4. 最终运行,计算出全局最小值 #####

输入:

	[optTheta,functionVal,exitFlag]=fminunc(@costFunction,initialTheta,options);
	% 这个代码就是把前面定义的东西都传入fminunc函数,调用一下然后得出结果而已 %




##  多元分类问题(一对多) ##

**和二元分类问题的区别就是,结果不只是两个,可能有多个.**

核心算法举例:

假设有1,2,3三个分类.

我们要做的事情是

1. 把(1)和(2,3)看作两个分类,然后当作二元分类处理,得出一个模型函数,这个模型函数计算的是:输入属于分类(1)的概率
2. 把(2)和(1,3)看作两个分类,然后当作二元分类处理,得出一个模型函数,这个模型函数计算的是:输入属于分类(2)的概率
3. 把(3)和(1,2)看作两个分类,然后当作二元分类处理,得出一个模型函数,这个模型函数计算的是:输入属于分类(3)的概率


然后现在要预测分类的时候,只要都计算一次每个分类的概率,找出最大的那个即为预测值.

